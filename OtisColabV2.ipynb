{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOPaRYKcYTX8BiWXRFJzeVT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OtisAlejandro/LLMColabs/blob/main/OtisColabV2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Otis' Colab v2\n",
        "Helo everyone! It's been a while since I updated my previous Colabs, and a lot of new models have come out, as well as a heap of changes. The goal of this colab is to have all of the newest and improved models readily available with an easy-to-use interface. I hope you enjoy and if you have any questions or concerns, feel free to DM me on Discord (Otis#9664) or ping me in the Pygmalion/TouHouAI Server.\n",
        "\n",
        "---\n",
        "\n",
        "## FAQ\n",
        "- Otis, where are the 6B Models? **6B models have been removed from this notebook because they do not run on max context size. They are an option on the 4Bit section, however they will not run as good as normal inference.**\n",
        "\n",
        "\n",
        "This Colab will have both an option for either normal Kobold (3Bs and smaller models) and 4Bit (Bigger models like 7Bs and 13Bs). Please make sure you have selected the correct model and are utilizing the correct cell before proceeding.\n",
        "\n",
        "\n",
        "Before selecting a model, please refer to the chart below to view each model and its recommended settings.\n",
        "\n",
        "# Models (FP16)\n",
        "| Model | Style | Recommended Settings |\n",
        "| --- | --- | --- |\n",
        "| New - [RedPajama Chat 3B](https://huggingface.co/togethercomputer/RedPajama-INCITE-Chat-3B-v1) by togethercomputer | Chat | Context Size: 2048, Generation Size: Up to 512 |\n",
        "| New - [RedPajama Instruct 3B](https://huggingface.co/togethercomputer/RedPajama-INCITE-Instruct-3B-v1) by togethercomputer | Instruct | Context Size: 2048, Generation Size: Up to 512 |\n",
        "| New - [RedPajama Base 3B](https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-3B-v1) by togethercomputer | Generic | Context Size: 2048, Generation Size: Up to 512 |\n",
        "| [Nerys 2.7B](https://huggingface.co/KoboldAI/fairseq-dense-2.7B-Nerys) by Mr. Seeker | Novel/Adventure | Context Size: 2048, Generation Size: Up to 512 |\n",
        "| [Erebus 2.7B](https://huggingface.co/KoboldAI/OPT-2.7B-Erebus) by Mr. Seeker | NSFW | Context Size: 2048, Generation Size: Up to 512 |\n",
        "| [Janeway 2.7B](https://huggingface.co/KoboldAI/GPT-Neo-2.7B-Janeway) by Mr. Seeker | Novel | Context Size: 2048, Generation Size: Up to 512\n",
        "| [Picard 2.7B](https://huggingface.co/KoboldAI/GPT-Neo-2.7B-Picard)by Mr. Seeker | Novel | Context Size: 2048, Generation Size: Up to 512 |\n",
        "| [AID 2.7B](https://huggingface.co/KoboldAI/GPT-Neo-2.7B-AID) by melastacho | Adventure | Context Size: 2048, Generation Size: Up to 512 |\n",
        "| [Horni LN 2.7B](https://huggingface.co/KoboldAI/GPT-Neo-2.7B-Horni-LN) by fintune | Novel | Context Size: 2048, Generation Size: Up to 512 |\n",
        "| [Horni 2.7B](https://huggingface.co/KoboldAI/GPT-Neo-2.7B-Horni) by finetune | NSFW | Context Size: 2048, Generation Size: Up to 512 |\n",
        "| [Shinen 2.7B](https://huggingface.co/KoboldAI/GPT-Neo-2.7B-Shinen) by Mr. Seeker | NSFW | Context Size: 2048, Generation Size: Up to 512 |\n",
        "| [OPT 2.7B](https://huggingface.co/facebook/opt-2.7b) by Metaseq | Generic | Context Size: 2048, Generation Size: Up to 512 |\n",
        "| [Fairseq Dense 2.7B](https://huggingface.co/KoboldAI/fairseq-dense-2.7B) | Generic | Context Size: 2048, Generation Size: Up to 512 |\n",
        "| [Neo 2.7B](https://huggingface.co/EleutherAI/gpt-neo-2.7B) by EleutherAI | Generic | Context Size: 2048, Generation Size: Up to 512\n",
        "\n",
        "---\n",
        "\n",
        "# Models (8bit/4Bit) - Nothing Yet ;)\n",
        "| Model | Style | Recommended Settings |\n",
        "| --- | --- | --- |\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "| Style     | Description                                                  |\n",
        "| --------- | ------------------------------------------------------------ |\n",
        "| Novel     | For regular story writing, not compatible with Adventure mode or other specialty modes. |\n",
        "| NSFW      | Indicates that the model is strongly biased towards NSFW content and is not suitable for children, work environments or livestreaming. Most NSFW models are also Novel models in nature. |\n",
        "| Adventure | These models are excellent for people willing to play KoboldAI like a Text Adventure game and are meant to be used with Adventure mode enabled. Even if you wish to use it as a Novel style model you should always have Adventure mode on and set it to story. These models typically have a strong bias towards the use of the word You and without Adventure mode enabled break the story flow and write actions on your behalf. |\n",
        "| Generic   | Generic models are not trained towards anything specific, typically used as a basis for other tasks and models. They can do everything the other models can do, but require much more handholding to work properly. Generic models are an ideal basis for tasks that we have no specific model for, or for experiencing a softprompt in its raw form. |\n"
      ],
      "metadata": {
        "id": "Q-UJDRY7U-8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <-- Tap this if you are on Mobile, and click play. { display-mode: \"form\" }\n",
        "%%html\n",
        "<b>Press play on the music player to keep the tab alive, then start KoboldAI below (Uses only 13MB of data)</b><br/>\n",
        "<audio src=\"https://raw.githubusercontent.com/KoboldAI/KoboldAI-Client/main/colab/silence.m4a\" controls>"
      ],
      "metadata": {
        "id": "530QDWtTcIIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IF YOU ARE NOT USING 8/4BIT"
      ],
      "metadata": {
        "id": "RfNjseJ3jBPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <b><-- Select your model and then click this to start KoboldAI if you are using a regular model.</b>\n",
        "\n",
        "Model = \"RedPajama Chat 3B\" #@param [\"RedPajama Chat 3B\", \"RedPajama Instruct 3B\", \"RedPajama Base 3B\", \"Nerys 2.7B\", \"Erebus 2.7B\", \"Janeway 2.7B\", \"Picard 2.7B\", \"AID 2.7B\", \"Horny LN 2.7B\", \"Horni 2.7B\", \"Shinen 2.7B\", \"OPT 2.7B\", \"Fairseq Dense 2.7B\", \"Neo 2,7B\"] {allow-input: true}\n",
        "Provider = \"Cloudflare\" #@param [\"Localtunnel\", \"Cloudflare\"]\n",
        "use_google_drive = False #@param {type:\"boolean\"}\n",
        "\n",
        "!nvidia-smi\n",
        "from google.colab import drive\n",
        "if use_google_drive:\n",
        "  drive.mount('/content/drive/')\n",
        "else:\n",
        "  import os\n",
        "  if not os.path.exists(\"/content/drive\"):\n",
        "    os.mkdir(\"/content/drive\")\n",
        "  if not os.path.exists(\"/content/drive/MyDrive/\"):\n",
        "    os.mkdir(\"/content/drive/MyDrive/\")\n",
        "\n",
        "Revision = \"\"\n",
        "\n",
        "if Model == \"RedPajama Chat 3B\":\n",
        "  Model = \"togethercomputer/RedPajama-INCITE-Chat-3B-v1\"\n",
        "  path = \"\"\n",
        "  download = \"\"\n",
        "  Version = \"United\"\n",
        "elif Model == \"RedPajama Instruct 3B\":\n",
        "  Model = \"togethercomputer/RedPajama-INCITE-Instruct-3B-v1\"\n",
        "  path = \"\"\n",
        "  download = \"\"\n",
        "  Version = \"United\"\n",
        "elif Model == \"RedPajama Base 3B\":\n",
        "  Model = \"togethercomputer/RedPajama-INCITE-Base-3B-v1\"\n",
        "  path = \"\"\n",
        "  download = \"\"\n",
        "  Version = \"United\"\n",
        "elif Model == \"Nerys 2.7B\":\n",
        "  Model = \"KoboldAI/fairseq-dense-2.7B-Nerys\"\n",
        "  path = \"\"\n",
        "  download = \"\"\n",
        "  Version = \"United\"\n",
        "elif Model == \"Erebus 2.7B\":\n",
        "  Model = \"KoboldAI/OPT-2.7B-Erebus\"\n",
        "  path = \"\"\n",
        "  download = \"\"\n",
        "  Version = \"United\"\n",
        "elif Model == \"Janeway 2.7B\":\n",
        "  Model = \"KoboldAI/GPT-Neo-2.7B-Janeway\"\n",
        "  path = \"\"\n",
        "  download = \"\"\n",
        "  Version = \"United\"\n",
        "elif Model == \"Picard 2.7B\":\n",
        "  Model = \"KoboldAI/GPT-Neo-2.7B-Picard\"\n",
        "  path = \"\"\n",
        "  download = \"\"\n",
        "  Version = \"United\"\n",
        "elif Model == \"AID 2.7B\":\n",
        "  Model = \"KoboldAI/GPT-Neo-2.7B-AID\"\n",
        "  path = \"\"\n",
        "  download = \"\"\n",
        "  Version = \"United\"\n",
        "elif Model == \"Horni LN 2.7B\":\n",
        "  Model = \"KoboldAI/GPT-Neo-2.7B-Horni-LN\"\n",
        "  path = \"\"\n",
        "  download = \"\"\n",
        "  Version = \"United\"\n",
        "elif Model == \"Horni 2.7B\":\n",
        "  Model = \"KoboldAI/GPT-Neo-2.7B-Horni\"\n",
        "  path = \"\"\n",
        "  download = \"\"\n",
        "  Version = \"United\"\n",
        "elif Model == \"Shinen 2.7B\":\n",
        "  Model = \"KoboldAI/GPT-Neo-2.7B-Shinen\"\n",
        "  path = \"\"\n",
        "  download = \"\"\n",
        "  Version = \"United\"\n",
        "elif Model == \"OPT 2.7B\":\n",
        "  Model = \"facebook/opt-2.7b\"\n",
        "  path = \"\"\n",
        "  download = \"\"\n",
        "  Version = \"United\"\n",
        "elif Model == \"Fairseq Dense 2.7B\":\n",
        "  Model = \"KoboldAI/fairseq-dense-2.7B\"\n",
        "  path = \"\"\n",
        "  download = \"\"\n",
        "  Version = \"United\"\n",
        "elif Model == \"Neo 2.7B\":\n",
        "  Model = \"EleutherAI/gpt-neo-2.7B\"\n",
        "  path = \"\"\n",
        "  download = \"\"\n",
        "  Version = \"United\"\n",
        "\n",
        "if Provider == \"Localtunnel\":\n",
        "  tunnel = \"--localtunnel yes\"\n",
        "else:\n",
        "  tunnel = \"\"\n",
        "\n",
        "!wget https://koboldai.org/ckds -O - | bash /dev/stdin -m $Model -g $Version $Revision $tunnel"
      ],
      "metadata": {
        "id": "7ln8lhX7cbz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FOR 8BIT/4BIT MODELS"
      ],
      "metadata": {
        "id": "slj4XzFFjErQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Clone the 4bit KoboldAI Repository and Download LLM (Will take ~7 Minutes to Load)\n",
        "#@markdown Select from a list of available 4bit models.\n",
        "\n",
        "Model = \"No Models Yet :(\" #@param [\"No Models Yet :(\"] {allow-input: true}\n",
        "#Clone the Repo\n",
        "!git clone https://github.com/0cc4m/KoboldAI -b latestgptq --recurse-submodules\n",
        "#Install Requirements\n",
        "!cd KoboldAI && ./install_requirements.sh cuda"
      ],
      "metadata": {
        "id": "2R0taNX1jG9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Start KoboldAI\n",
        "Tunnel = \"Localtunnel\" #@param [\"Localtunnel\",\"Cloudflare\"] {allow-input: true}\n",
        "if Tunnel == \"Localtunnel\":\n",
        "  !npm install -g localtunnel\n",
        "  !cd KoboldAI && ./play.sh --localtunnel --quiet\n",
        "else:\n",
        "  !cd KoboldAI && ./play.sh --remote --quiet"
      ],
      "metadata": {
        "id": "81udpD2_lN72"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}