{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OtisAlejandro/LLMColabs/blob/main/Otis'_4Bit_Kobold_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Welcome to Otis' 4bit Kobold Colab!\n",
        "Mady by OshieteKudasai, Peepy, and Otis\n",
        "Hello everyone and thank you for choosing my colab. If you want to run 13B models but you do not have a good GPU or TPUs aren't working, you can run any compatible 13B model here! This notebook uses Occ4m's 4Bit Kobold Fork which can be foud [here.](https://github.com/0cc4m/KoboldAI)\n",
        "\n",
        "If you'd like more information about Kobold, you can read their Github's readme file here: https://github.com/KoboldAI/KoboldAI-Client/blob/main/readme.md\n",
        "\n",
        "TPUs are currently unavailable, but I'll be making a TPU edition as well for the larger models.\n",
        "\n",
        "If you have any questions or concerns, feel free to DM me @Otis#9664 or ping me in the [PygmalionAI Discord.](discord.gg/pygmalionai)\n",
        "\n",
        "**A special thanks to Peepy and OshieteKudasai for their contributions to this notebook! <3**\n",
        "\n",
        "---\n",
        "## How to load the KoboldAI UI\n",
        "1. Although I designed the colab for desktop users, it should not have a problem on mobile as long as you are using desktop mode. If you're on mobile, tap the play button on the first cell so your Colab doesn't get shut down for inactivity.\n",
        "2. Next, run the second cell to clone the GitHub repository. This will take ~4 minutes to complete, and the cell will stop running automatically.\n",
        "3. Run the 3rd cell to download the model directly from the repo. For now, only Koala is available, however I plan to implement a model selector.\n",
        "4. Run KoboldAI, go to the new_ui. Click interface, and enable Experimental UI. Then, go back to the first page and click 'Load Model'. Select 'load from directory' and select the model you wish to run. After that's done, select 4bit, not 8bit, and wait for it to load! Plug your KoboldAI cloudflare link into your preferred UI, and Chat with 2048 context size, and 12 t/s!\n",
        "4. If you get a message saying no accelerator is available/you ran out of runtime, you are gonna need to wait 24 hours for your cooldown to reset, or try again.\n",
        "5. After everything has loaded, you will get a link to the KoboldAI UI that you can also copy into TavernAI's API and use there. If you get a warning for the localtunnel link, just acknowledge it and proceed to the page. If you get a 1033 error with the Cloudflare link, wait 1 minute and refresh the error page.\n",
        "\n",
        "---\n",
        "\n",
        "Make sure to keep this page open while you are using KoboldAI, and check back regularly to see if you got a Captcha. Failure to complete the captcha's in time can result in termination of your session or a lower priority towards the TPUs.\n",
        "\n",
        "Firefox users need to disable the enhanced tracking protection or use a different browser in order to be able to use Google Colab without errors (This is not something we can do anything about, the cookie blocker breaks the Google Drive integration because it uses different domains).\n",
        "\n",
        "This colab was written completely by hand by Peepy, OshieteKudasai, and Otis. Any copies, or reproductions of this notebook **must** include credit."
      ],
      "metadata": {
        "id": "re1s6eSjDG3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <-- Tap this if you play on Mobile { display-mode: \"form\" }\n",
        "%%html\n",
        "<b>Press play on the music player to keep the tab alive, then start KoboldAI below (Uses only 13MB of data)</b><br/>\n",
        "<audio src=\"https://raw.githubusercontent.com/KoboldAI/KoboldAI-Client/main/colab/silence.m4a\" controls>"
      ],
      "metadata": {
        "id": "MQvp5Ao1FL01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Clone the 4bit KoboldAI Repository and Download LLM (Will take ~7 Minutes to Load)\n",
        "#@markdown Select from a list of available 4bit models.\n",
        "\n",
        "Model = \"Pygmalion 6B 4Bit\" #@param [\"Koala 13B\", \"OpenAssistant 13B (LLaMA)\", \"Pygmalion 6B 4Bit\", \"Llama 13b SUPERCOT 4Bit\", \"Lotus 12B 4Bit",\"GPT4 x Alpaca 13B RolePlayLora 4Bit-v2\", \"GPT4 x Alpaca 13B Lora 4Bit\", \"GPT4 x Alpaca 13B Native 4Bit\"] {allow-input: true}\n",
        "\n",
        "# Clone the repository\n",
        "!git clone https://github.com/0cc4m/KoboldAI -b latestgptq --recurse-submodules\n",
        "#Enter Kobold Dir and run IR with cuda parameter.\n",
        "!cd KoboldAI && ./install_requirements.sh cuda\n",
        "\n",
        "!apt install aria2\n",
        "if Model == \"Koala 13B\":\n",
        "  !cd KoboldAI/models && mkdir koala-13B-GPTQ-4bit-128g && cd koala-13B-GPTQ-4bit-128g && aria2c -Z -j4 --summary-interval=5 https://huggingface.co/TheBloke/koala-13B-GPTQ-4bit-128g/resolve/main/.gitattributes https://huggingface.co/TheBloke/koala-13B-GPTQ-4bit-128g/resolve/main/README.md https://huggingface.co/TheBloke/koala-13B-GPTQ-4bit-128g/resolve/main/config.json https://huggingface.co/TheBloke/koala-13B-GPTQ-4bit-128g/resolve/main/generation_config.json https://huggingface.co/TheBloke/koala-13B-GPTQ-4bit-128g/resolve/main/pytorch_model.bin.index.json https://huggingface.co/TheBloke/koala-13B-GPTQ-4bit-128g/resolve/main/special_tokens_map.json https://huggingface.co/TheBloke/koala-13B-GPTQ-4bit-128g/resolve/main/tokenizer.model https://huggingface.co/TheBloke/koala-13B-GPTQ-4bit-128g/resolve/main/tokenizer_config.json https://huggingface.co/TheBloke/koala-13B-GPTQ-4bit-128g/resolve/main/koala-13B-4bit-128g.no-act-order.ooba.pt && mv 9e556afd44213b6bd1be2b850ebbbd98f5481437a8021afaf58ee7fb1818d347 tokenizer.model && mv e80a5b1fdb63334c5d05f0ca3ad756da00937213e7c04ffa554e234f847d290f 4bit-128g.pt \n",
        "elif Model == \"OpenAssistant 13B (LLaMA)\":\n",
        "  !cd KoboldAI/models && mkdir oasst-llama13b-4bit-128g && cd oasst-llama13b-4bit-128g && aria2c -Z -j4 --summary-interval=5 https://huggingface.co/gozfarb/oasst-llama13b-4bit-128g/resolve/main/.gitattributes https://huggingface.co/gozfarb/oasst-llama13b-4bit-128g/resolve/main/README.md https://huggingface.co/gozfarb/oasst-llama13b-4bit-128g/resolve/main/added_tokens.json https://huggingface.co/gozfarb/oasst-llama13b-4bit-128g/resolve/main/config.json https://huggingface.co/gozfarb/oasst-llama13b-4bit-128g/resolve/main/generation_config.json https://huggingface.co/gozfarb/oasst-llama13b-4bit-128g/resolve/main/oasst-llama13b-4bit-128g.safetensors https://huggingface.co/gozfarb/oasst-llama13b-4bit-128g/resolve/main/pytorch_model.bin.index.json https://huggingface.co/gozfarb/oasst-llama13b-4bit-128g/resolve/main/special_tokens_map.json https://huggingface.co/gozfarb/oasst-llama13b-4bit-128g/resolve/main/tokenizer.model https://huggingface.co/gozfarb/oasst-llama13b-4bit-128g/resolve/main/tokenizer_config.json && mv 9e556afd44213b6bd1be2b850ebbbd98f5481437a8021afaf58ee7fb1818d347 tokenizer.model && mv 5d9f2ca80722b034f3d3534c45b6498d6c8329f882766f6516e71b97edbd764a 4bit-128g.safetensors \n",
        "elif Model == \"Pygmalion 6B 4Bit\":\n",
        "  !cd KoboldAI/models && mkdir pygmalion-6b-4bit-128g && cd pygmalion-6b-4bit-128g && aria2c -Z -j4 --summary-interval=5 https://huggingface.co/mayaeary/pygmalion-6b-4bit-128g/resolve/main/.gitattributes https://huggingface.co/mayaeary/pygmalion-6b-4bit-128g/resolve/main/README.md https://huggingface.co/mayaeary/pygmalion-6b-4bit-128g/resolve/main/added_tokens.json https://huggingface.co/mayaeary/pygmalion-6b-4bit-128g/resolve/main/config.json https://huggingface.co/mayaeary/pygmalion-6b-4bit-128g/resolve/main/merges.txt https://huggingface.co/mayaeary/pygmalion-6b-4bit-128g/resolve/main/pygmalion-6b-4bit-128g.safetensors https://huggingface.co/mayaeary/pygmalion-6b-4bit-128g/resolve/main/special_tokens_map.json https://huggingface.co/mayaeary/pygmalion-6b-4bit-128g/resolve/main/tokenizer.json https://huggingface.co/mayaeary/pygmalion-6b-4bit-128g/resolve/main/tokenizer_config.json https://huggingface.co/mayaeary/pygmalion-6b-4bit-128g/resolve/main/vocab.json && mv c364f668577620596cc31556a2b642bb93bad406aa9d71b40e25cd739cb5d875 4bit-128g.safetensors \n",
        "elif Model == \"Llama 13b SUPERCOT 4Bit\":\n",
        "  !cd KoboldAI/models && mkdir llama-13b-supercot-4bit-128g && cd llama-13b-supercot-4bit-128g && aria2c -Z -j4 --summary-interval=5 https://huggingface.co/ausboss/llama-13b-supercot-4bit-128g/resolve/main/.gitattributes https://huggingface.co/ausboss/llama-13b-supercot-4bit-128g/resolve/main/README.md https://huggingface.co/ausboss/llama-13b-supercot-4bit-128g/resolve/main/config.json https://huggingface.co/ausboss/llama-13b-supercot-4bit-128g/resolve/main/generation_config.json https://huggingface.co/ausboss/llama-13b-supercot-4bit-128g/resolve/main/llama-13b-supercot-4bit-128g-cuda.safetensors https://huggingface.co/ausboss/llama-13b-supercot-4bit-128g/resolve/main/pytorch_model.bin.index.json https://huggingface.co/ausboss/llama-13b-supercot-4bit-128g/resolve/main/pytorch_model.bin.index.json.1 https://huggingface.co/ausboss/llama-13b-supercot-4bit-128g/resolve/main/special_tokens_map.json https://huggingface.co/ausboss/llama-13b-supercot-4bit-128g/resolve/main/tokenizer.json https://huggingface.co/ausboss/llama-13b-supercot-4bit-128g/resolve/main/tokenizer.model https://huggingface.co/ausboss/llama-13b-supercot-4bit-128g/resolve/main/tokenizer_config.json && mv 9e556afd44213b6bd1be2b850ebbbd98f5481437a8021afaf58ee7fb1818d347 tokenizer.model && mv 9f5db20716e8667e9922690d5f3a534c59a30193422f95a9d62c4b567be4112a 4bit-128g.safetensors \n",
        "elif Model == \"Lotus 12B 4Bit":\"\n",
        "  !cd KoboldAI/models && mkdir lotus-12b-gptqv2-4bit && cd lotus-12b-gptqv2-4bit && aria2c -Z -j4 --summary-interval=5 https://huggingface.co/hakurei/lotus-12B/resolve/main/.gitattributes https://huggingface.co/hakurei/lotus-12B/resolve/main/README.md https://huggingface.co/hakurei/lotus-12B/resolve/main/config.json https://huggingface.co/hakurei/lotus-12B/resolve/main/special_tokens_map.json https://huggingface.co/hakurei/lotus-12B/resolve/main/tokenizer.json https://huggingface.co/hakurei/lotus-12B/resolve/main/tokenizer_config.json https://huggingface.co/hakurei/lotus-12B/blob/main/pytorch_model.bin.index.json https://huggingface.co/autobots/lotus-12b-gptqv2-4bit/resolve/main/.gitattributes https://huggingface.co/autobots/lotus-12b-gptqv2-4bit/resolve/main/README.md https://huggingface.co/autobots/lotus-12b-gptqv2-4bit/resolve/main/lotus-12b-4bit-V2-fr-act-truseq.safetensors && mv 8235ee66fad41d6d0e215e6f43e718a4d6a4f4252d769ccacff63793208defda 4bit.safetensors \n",
        "elif Model == \"GPT4 x Alpaca 13B RolePlayLora 4Bit-v2\":\n",
        "  !cd KoboldAI/models && mkdir GPT4-x-Alpaca13b-RolePlayLora-4bit-v2 && cd GPT4-x-Alpaca13b-RolePlayLora-4bit-v2 && aria2c -Z -j4 --summary-interval=5 https://huggingface.co/teknium/GPT4-x-Alpaca13b-RolePlayLora-4bit-v2/resolve/main/.gitattributes https://huggingface.co/teknium/GPT4-x-Alpaca13b-RolePlayLora-4bit-v2/reolve/main/README.md https://huggingface.co/teknium/GPT4-x-Alpaca13b-RolePlayLora-4bit-v2/resolve/main/added_tokens.json https://huggingface.co/teknium/GPT4-x-Alpaca13b-RolePlayLora-4bit-v2/resolve/main/config.json https://huggingface.co/teknium/GPT4-x-Alpaca13b-RolePlayLora-4bit-v2/resolve/main/generation_config.json https://huggingface.co/teknium/GPT4-x-Alpaca13b-RolePlayLora-4bit-v2/resolve/main/gpt4xalpacarplorav2-4bit.safetensors https://huggingface.co/teknium/GPT4-x-Alpaca13b-RolePlayLora-4bit-v2/resolve/main/pytorch_model.bin.index.json https://huggingface.co/teknium/GPT4-x-Alpaca13b-RolePlayLora-4bit-v2/resolve/main/special_tokens_map.json https://huggingface.co/teknium/GPT4-x-Alpaca13b-RolePlayLora-4bit-v2/resolve/main/tokenizer.model https://huggingface.co/teknium/GPT4-x-Alpaca13b-RolePlayLora-4bit-v2/resolve/main/tokenizer_config.json https://huggingface.co/teknium/GPT4-x-Alpaca13b-RolePlayLora-4bit-v2/resolve/main/trainer_state.json https://huggingface.co/teknium/GPT4-x-Alpaca13b-RolePlayLora-4bit-v2/resolve/main/training_args.bin && mv 93499e6d2b72f466e8db31be3266bf88f883698f46eeef49a084987dc0e7e0de training_args_bin && mv 9e556afd44213b6bd1be2b850ebbbd98f5481437a8021afaf58ee7fb1818d347 tokenizer.model && mv bf79bae918123f547cd7768f6f5d86c351e80e7a63136c54dc853e7c2ca7cc6f 4bit.safetensors \n",
        "elif Model == \"GPT4 x Alpaca 13B Lora 4Bit\":\n",
        "  !cd KoboldAI/models && mkdir gpt4-alpaca-lora-13B-GPTQ-4bit-128g && cd gpt4-alpaca-lora-13B-GPTQ-4bit-128g && aria2c -Z -j4 --summary-interval=5 https://huggingface.co/TheBloke/gpt4-alpaca-lora-13B-GPTQ-4bit-128g/resolve/main/.gitattributes https://huggingface.co/TheBloke/gpt4-alpaca-lora-13B-GPTQ-4bit-128g/resolve/main/README.md https://huggingface.co/TheBloke/gpt4-alpaca-lora-13B-GPTQ-4bit-128g/resolve/main/config.json https://huggingface.co/TheBloke/gpt4-alpaca-lora-13B-GPTQ-4bit-128g/resolve/main/generation_config.json https://huggingface.co/TheBloke/gpt4-alpaca-lora-13B-GPTQ-4bit-128g/resolve/main/gpt4-alpaca-lora-13B-GPTQ-4bit-128g.safetensors https://huggingface.co/TheBloke/gpt4-alpaca-lora-13B-GPTQ-4bit-128g/resovle/main/pytorch_model.bin.index.json https://huggingface.co/TheBloke/gpt4-alpaca-lora-13B-GPTQ-4bit-128g/resolve/main/special_tokens_map.json https://huggingface.co/TheBloke/gpt4-alpaca-lora-13B-GPTQ-4bit-128g/resolve/main/tokenizer.model https://huggingface.co/TheBloke/gpt4-alpaca-lora-13B-GPTQ-4bit-128g/resolve/main/tokenizer_config.json && mv 9e556afd44213b6bd1be2b850ebbbd98f5481437a8021afaf58ee7fb1818d347 tokenizer.model && mv 7235e7cd46ba0d49c3b40c8f870f7f1adc78cf52830e64147ecbe91e5fb7975a 4bit-128g.safetensors \n",
        "elif Model == \"GPT4 x Alpaca 13B Native 4Bit\":\n",
        "  !cd KoboldAI/models && mkdir gpt4-x-alpaca-13b-native-4bit-128g && cd gpt4-x-alpaca-13b-native-4bit-128g && aria2c -Z -j4 --summary-interval=5 https://huggingface.co/4bit/gpt4-x-alpaca-13b-native-4bit-128g-cuda/resolve/main/.gitattributes https://huggingface.co/4bit/gpt4-x-alpaca-13b-native-4bit-128g-cuda/resolve/main/4bit-128g.safetensors https://huggingface.co/4bit/gpt4-x-alpaca-13b-native-4bit-128g-cuda/resolve/main/README.md https://huggingface.co/4bit/gpt4-x-alpaca-13b-native-4bit-128g-cuda/resolve/main/added_tokens.json https://huggingface.co/4bit/gpt4-x-alpaca-13b-native-4bit-128g-cuda/resolve/main/config.json https://huggingface.co/4bit/gpt4-x-alpaca-13b-native-4bit-128g-cuda/resolve/main/generation_config.json https://huggingface.co/4bit/gpt4-x-alpaca-13b-native-4bit-128g-cuda/resolve/main/pytorch_model.bin.index.json https://huggingface.co/4bit/gpt4-x-alpaca-13b-native-4bit-128g-cuda/resolve/main/special_tokens_map.json https://huggingface.co/4bit/gpt4-x-alpaca-13b-native-4bit-128g-cuda/resolve/main/tokenizer.model https://huggingface.co/4bit/gpt4-x-alpaca-13b-native-4bit-128g-cuda/resolve/main/tokenizer_config.json https://huggingface.co/4bit/gpt4-x-alpaca-13b-native-4bit-128g-cuda/resolve/main/trainer_state.json https://huggingface.co/4bit/gpt4-x-alpaca-13b-native-4bit-128g-cuda/resolve/main/training_args.bin && mv 93499e6d2b72f466e8db31be3266bf88f883698f46eeef49a084987dc0e7e0de training_args.bin && mv 9e556afd44213b6bd1be2b850ebbbd98f5481437a8021afaf58ee7fb1818d347 tokenizer.mobel && mv e712275efafc9d813909799db0b47a7bfc1646ba9e4f52850ce84f749ae73b74 4bit-128g.safetensors
      ],
      "metadata": {
        "id": "vN1jp0sKEpF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Launch Kobold AI (Go to new_ui link and enable Experimental UI in interface settings, then load model in 4bit.)\n",
        "#Enter KAI dir, launch shell script with remote parameter for CloudFlare links\n",
        "!cd KoboldAI && ./play.sh --remote"
      ],
      "metadata": {
        "id": "e2KO63T5FjlG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
